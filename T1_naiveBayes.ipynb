{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ML/Python Course Material\n",
    "### [Mohamad Dia](http://mohamaddia.me)\n",
    "Feb 11 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1: Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "* Understand the general intuition behind naive Bayes classifier.\n",
    "<br>\n",
    "* Apply Bernoulli naive Bayes on real-world text classification problem.\n",
    "<br>\n",
    "* Know the strengths and limitations of naive Bayes.\n",
    "<br>\n",
    "* Implement, in python, two instances of the naive Bayes classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "* Basic probability theory and statistics.\n",
    "<br>\n",
    "* Basic python programming (no need to be familiar with high level ML libraries such as scikit-learn).\n",
    "<br>\n",
    "* General machine learning knowledge (training vs testing, over-fitting, ...) is useful but not required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "* Python Data Science Handbook, Jake VanderPlas, O'Reilly Media 2016.\n",
    "<br>\n",
    "* Adaptation and Learning, Ali H. Sayed, EPFL 2014.\n",
    "<br>\n",
    "* Advanced Data Science Course, Robert J. Brunner, UIUC 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Introduction: Naive Bayes\n",
    "Naive Bayes classifier is one of the most straightforward, yet efficient, classifiers used in supervised machine learning.\n",
    "Since it is simple and fast to train, naive Bayes is one of the first classifiers to be considered as a baseline for many classification problems. In particular, it is used in text classification (e.g. spam filtering, sentiment analysis,...).\n",
    "\n",
    "In spite of its simplicity, naive Bayes has shown to be very efficient in a variety of real-world problems. Hence, it is essential to learn this type of classifiers and get to know its strengths and limitations compared to other classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. From Bayes to Naive Bayes\n",
    "\n",
    "Naive Bayes classifier is called so because it is a probabilistic classifier that uses the **Bayes probability theorem** in order to predict the class probability of a certain data input (or feature vector). Moreover, naive Bayes classifier makes some over-simplified (or **naive**) assumptions regarding the relationship between the features that define the input.\n",
    "\n",
    "Let's assume we are interested in classifying newspaper articles into $K$ different classes: $C_1, C_2, ..., C_K$. You can think of theses classes being the themes (politics, finance, sports, ...). The classification is based on some feature vector $\\mathbf{x}$ of length $n$ for each article, with $\\mathbf{x} = (x_1, x_2,...,x_n)$. For example, you can think of theses features as boolean variables determining whether a certain word exists in the article or not (e.g. $x_1 = 1$ if the word \"election\" exists in the article and $x_1 = 0$ otherwise, $x_2$ for the word \"parliament\", $x_3$ for \"football\", ...).\n",
    "\n",
    "Naive Bayes tries to calculate the conditional probabilities of the classes based on the feature vector $\\mathbf{x}$:\n",
    "\n",
    "$$P(C_k \\mid \\mathbf{x}) \\quad \\text{for} \\,\\, k = 1,...,K.$$\n",
    "\n",
    "These conditional probabilities are called the *posterior probabilities*. Given a certain article represented by the feature vector $\\mathbf{x}$, the classifier computes the posterior probabilities for all the $K$ classes then decide on the winning class $\\widehat{C}$ by comparing them according to the following *maximum a posteriori* (MAP) rule:\n",
    "\n",
    "$$\\widehat{C} = \\arg\\max_k P(C_k \\mid \\mathbf{x}).$$\n",
    "\n",
    "By applying Bayes probability theorem, we know that the posterior probability can be rewritten as such:\n",
    "\n",
    "$$P(C_k \\mid \\mathbf{x}) = P(\\mathbf{x} \\mid C_k) P(C_k) / P(\\mathbf{x}),$$\n",
    "\n",
    "where $P(\\mathbf{x} \\mid C_k)$ is denoted as the *likelihood* and $P(C_k)$ as the class *prior*. Since the denominator $P(\\mathbf{x})$ is independent of all the classes, one can rewrite the MAP rule as such:\n",
    "\n",
    "$$\\widehat{C} = \\arg\\max_k P(C_k \\mid \\mathbf{x}) = \\arg\\max_k P(\\mathbf{x} \\mid C_k) P(C_k).$$\n",
    "\n",
    "So far, we have defined what is called a *Bayesian classifier* based on some probabilities to be learned during training time. In the next, we will see that learning the class priors $P(C_k)$ is an easy task. However, learning the probability distributions of the likelihoods $P(\\mathbf{x} \\mid C_k)$ is a more complicated task. This is where the *naive* assumptions come into the play to simplify our life.\n",
    "\n",
    "For the class priors, we can use the frequency of each class in the training dataset in order to estimate $P(C_k)$ for $k = 1,...,K$. For example in a training dataset of $1000$ articles where $600$ of them are labeled under the class $C_1 =$ politics. We can estimate $P(C_1) = 600/1000 = 0.6$. The same applies to all the other classes. What remains is to define probability models (or distributions) so that we can compute the likelihoods $P(\\mathbf{x} \\mid C_k)$. In order to simplify this task, naive Bayes classifier makes the following assumption.\n",
    "It assumes that the features in the vector $\\mathbf{x}$ are **conditionally independent** on the class (i.e. for a given class the features do not interact), which allows us to rewrite the likelihood in this form:\n",
    "\n",
    "$$P(\\mathbf{x} \\mid C_k) = P(x_1,x_2,...,x_n \\mid C_k) \\doteq P(x_1\\mid C_k)\\times P(x_2\\mid C_k)\\times ... \\times P(x_n\\mid C_k) = \\prod_{i=1}^{n} P(x_i\\mid C_k),$$\n",
    "\n",
    "where the dotted equality above $(\\doteq)$ points to the conditional independence assumption. Note that this is a naive assumption that rarely holds in real data, hence the name of the classifier. Nonetheless, the naive Bayes classifier continues to give a good performance in a variety of applications.\n",
    "\n",
    "<!---\n",
    "> **Note on independence and conditional independence:** Let $A$, $B$, and $C$ be three random variables.\n",
    "* We say that $A$ and $B$ are independent if and only if $P(A B) = P(A)\\times P(B)$.\n",
    "* We say that $A$ and $B$ are conditionally independent on $C$ if and only if $P(A B\\mid C) = P(A\\mid C)\\times P(B\\mid C)$.\n",
    "<br>\n",
    "Independence is a stronger property than [conditional independence](https://en.wikipedia.org/wiki/Conditional_independence). If the former holds, then the latter holds (the opposite is not always true).--->\n",
    "\n",
    "The next step is to come up with a model (or probability distribution) for each term $P(x_i\\mid C_k)$. Such distributions highly depend on the type of the features we have (whether they are discrete, boolean, categorical, continuous,...). For this reason, there exist different models of the naive Bayes classifier and one should choose the one that suits the type of features at hand.\n",
    "\n",
    "For boolean features as in our running example on text classification, one can assume that $P(x_i\\mid C_k)$ follows a Bernoulli distribution and hence use the *Bernoulli naive Bayes* classifier. If the features represent the word counts in some [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) encoding, one can use the *multinomial naive Bayes* classifier. On the other hand, if the features are continuous as in images, one can use *Gaussian Naive Bayes* Classifier. These are the most common models of the naive Bayes classifier. For additional details and more models, check this scikit learn documentation [page](https://scikit-learn.org/stable/modules/naive_bayes.html). In the next, we will illustrate the Bernoulli naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Bernoulli Naive Bayes with application to text classification\n",
    "\n",
    "For boolean features, as in our text classification example where each feature indicates the presence or absence of a certain word, one can assume that the likelihood of each feature $P(x_i\\mid C_k)$ follows a Bernoulli distribution with a certain success probability $p_{ik}$: \n",
    "    \n",
    "$$P(x_i\\mid C_k) = \\begin{cases} p_{ik} \\qquad \\, \\, \\, \\text{if} \\quad x_i =1 \\\\ 1- p_{ik} \\quad \\text{if} \\quad x_i =0 \\end{cases}$$\n",
    "\n",
    "Hence, one can write $P(x_i\\mid C_k)$ as\n",
    "\n",
    "$$P(x_i\\mid C_k) = p_{ik}^{x_i} + (1-p_{ik})^{1-x_i}.$$\n",
    "\n",
    "The success probabilities are unknown so far, thus we need to learn them from the training dataset. Let's see how to do this using a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Example: Newspaper articles classification</u>\n",
    "\n",
    "Assume we have a training dataset of $T=6$ different articles labeled by one of two classes, \"P\" for \"politics\" and \"S\" for \"sports\". Each of these articles is represented by a boolean feature vector of length $n = 4$ where $x_1$ indicates whether the word \"election\" is present in the article, $x_2$ for the word \"parliament\", $x_3$ for the word \"player\", and $x_4$ for the word \"game\". For example, in the table below you can see that the first article (first row) is labeled as \"politics\" and only the word \"election\" is present.\n",
    "\n",
    "| election | parliament | player | game | Label |\n",
    "|:--------:|:----------:|:------:|:----:|:-----:|\n",
    "|     1    |      0     |    0   |   0  |   P   |\n",
    "|     0    |      0     |    1   |   1  |   S   |\n",
    "|     1    |      1     |    1   |   0  |   P   |\n",
    "|     1    |      0     |    1   |   1  |   S   |\n",
    "|     1    |      1     |    0   |   0  |   P   |\n",
    "|     0    |      1     |    0   |   1  |   P   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Training text classifier with a Bernoulli naive Bayes: </u>\n",
    "\n",
    "* First, we need to learn the class priors. Since we have $4$ out of $6$ training examples labeled as \"politics\" and $2$ as \"sports\", we can estimate $P(C_1) = 4/6$ and  $P(C_2) = 2/6$ (where $C_1$ indicates the class \"politics\" and $C_2$ \"sports\").\n",
    "\n",
    "* Second, we need to learn the likelihood parameters, i.e. the success probability $p_{ik}$ for each feature $i$ conditioned on the class $k$. Note that we have $K=2$ classes and $n=4$ features, hence we need to learn $2\\times 4$ different parameters. For example, $p_{11}$ is the probability of the word \"election\" being in a \"politics\" article, $p_{21}$ the probability of the word \"parliament\" in a \"politics\" article, ..., $p_{42}$ the probability of the word \"game\" in a \"sports\" article. To estimate $p_{11}$, we can count how many articles are labeled as \"politics\". We have $4$ of them. Among these $4$ politics articles, we have 3 that contains the word \"election\". Hence, we can estimate $p_{11} = 3/4$. We can do the same for the other parameters to get $p_{21} = 3/4$, $p_{31} = 1/4$, $p_{41} = 1/4$, $p_{12} = 1/2$, $p_{22} = 0$, $p_{32} = 1$, and $p_{42} = 1$. Note that these counting estimates are nothing but the **maximum likelihood estimates** of our model based on the training dataset. There are some variations of these estimates that help avoiding the extreme cases of having $0$ or $1$ probabilities and hence running into over-fitting. One of these techniques is called the [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Prediction using Bernoulli naive Bayes:</u>\n",
    "\n",
    "After training, let's assume we have received the following feature vector $[0,1,1,0]$ of a new article that we need to decide. This article contains the words \"parliament\" and \"player\". We need to apply the maximum a posteriori (MAP) rule and pick the class with the highest score.\n",
    "\n",
    "* $P(\\mathbf{x} \\mid C_1) P(C_1) = \\big(\\prod_{i=1}^4 P(x_i \\mid C_1)\\big) P(C_1) = \\big(\\prod_{i=1}^4 p_{i1}^{x_i} + (1-p_{i1})^{1-x_i}\\big) P(C_1) = (1-3/4)(3/4)(1/4)(1-1/4)(4/6) = 0.0234375$\n",
    "\n",
    "* $P(\\mathbf{x} \\mid C_2) P(C_2) = \\big(\\prod_{i=1}^4 P(x_i \\mid C_2)\\big) P(C_2) = \\big(\\prod_{i=1}^4 p_{i2}^{x_i} + (1-p_{i2})^{1-x_i}\\big) P(C_2) = (1-1/2)(0)(1)(1-1)(2/6) = 0$\n",
    "\n",
    "Therefore, our prediction is for \"politics\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Other Naive Bayes Models\n",
    "\n",
    "Many other models exist for naive Bayes classifier. Such models depend on the type of the features used. We refer to this [reading](https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html) for more details and python implementations on multinomial and Gaussian naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Summary\n",
    "<img src=\"summary.png\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Pros and Cons of Naive Bayes Classifier\n",
    "<img src=\"prosCons.png\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Programming Exercises\n",
    "\n",
    "See the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
