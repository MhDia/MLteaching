{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ML/Python Course Material\n",
    "### [Mohamad Dia](http://mohamaddia.me)\n",
    "Feb 11 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T3: Common Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Do we always need to normalize our data?\n",
    "Data normalization (or feature scaling) is an essential preprocessing technique in machine learning that makes the learning algorithm numerically more stable by brining all the features to the same scale. Whether to normalize the data or not depends on the type and scale of the features in addition to the choice of the machine learning model.\n",
    "\n",
    "**Tree-based** learning models for example (such as decision trees) do not require data normalization because the decision threshold can be learned independently for each feature regardless of its scale or range. **Graph-based** models (such as Naive Bayes) are designed to treat each feature independently and does not require normalization. However, normalizing the data won't really hurt.\n",
    "\n",
    "On the other hand, **gradient-based** models (e.g. logistic regression, neural network,...) and **distance-based** models (e.g. KNN) require data normalization when the features have different scales or units. Take for example the binary classification problem using logistic regression based on three features: the height of a person in cm, the weight in kg, and the annual salary in CHF. Since the features have completely different scales, the learning algorithm will be very sensitive and numerically unstable with many inefficient oscillations (a small change in the gradient of the third feature can have a huge impact compared to a similar change in the gradient of the first feature). Therefore, if we assume that different features have relatively similar importance in the classification, it is very essential to normalize the data so that we attain the following:\n",
    "* Numerical stability in the learning algorithm\n",
    "* Fast convergence\n",
    "* Learning algorithms can focus on learning the absolute importance of each feature (instead of learning its scale)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. When should we use min-max, or zero mean/unit variance scaling, what's the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero mean/unit variance scaling (or standardization) rescale the features so that they have the property of the standard normal distribution centered around $0$ with standard deviation equals $1$.\n",
    "\n",
    "Min-max scaling (or simply called normalization) brings all the feature values to the range $[0,1]$.\n",
    "\n",
    "Standardization or normalization? No absolute answer, it depends on the application. In general, most machine learning algorithm benefit from standardization more than normalization. However, there are cases where normalization is preferred.\n",
    "\n",
    "Standardization works better whenever there are outliers in the data (standardization makes the data unbounded while normalization bound the data). Moreover, whenever the learning algorithm assume normality (Gaussianity) of the features, we use standardization. For example, some neural networks with $\\tanh$ activations assume that the features are centered around zero, and hence it is better to standardize the data. However, this is not always the case. There are cases where the Gaussian assumption does not hold (e.g. image classification in neural network with $relu$ activation), hence one can use normalization. Furthermore, there are cases where the standard deviation is very small, which makes normalization a better option than standardization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
