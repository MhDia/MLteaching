{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ML/Python Course Material\n",
    "### [Mohamad Dia](http://mohamaddia.me)\n",
    "Feb 11 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2: F-1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "* Understand why the accuracy metric is not always enough to evaluate the performance on a real-world example (corona virus detection).\n",
    "<br>\n",
    "* Learn how to compute the recall, precision, F-1 score and understand the intuition behind these metrics.\n",
    "<br>\n",
    "* Learn how to use scikit-learn for performance evaluation via a coding demo on the breast cancer dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "* Basic knowledge in machine learning (binary classification, logistic regression, SVM, ...).\n",
    "<br>\n",
    "* Familiarity with Python and Scikit-learn library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "* Performance Evaluation in Machine Learning: The Good, The Bad, The Ugly and The Way Forward, P. Flach, AAAI 2019.\n",
    "<br>\n",
    "* The Relationship Between Precision-Recall and ROC Curves, J. Davis and M. Goadrich, ICML 2006.\n",
    "<br>\n",
    "* The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets, T. Saito and M. Rehmsmeier, PLoS ONE 2015.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Performance Evaluation\n",
    "Evaluating the performance of any machine learning algorithm is a very essential task. Consider a binary classification problem where you have trained a certain classifier on the training dataset, and then you perform the prediction on the test dataset. The main question that arises is how good is your model? How to assess the testing performance? And what kind of metric to use?\n",
    "\n",
    "A very common and intuitive thing to do is to measure the accuracy of the prediction by computing the ratio of the correctly predicted instances to the total number of instances in the test dataset. For example, assume you are doing binary classification for images of cats vs. dogs. After training, you test your model on 100 images. Your classifier outputs 55 cats (among them 50 are correct) and 45 dogs (among them 43 are correct). Hence, the accuracy of your classifier in $(50+43)/100 = 93\\%$ and the error rate is $7\\%$. In this particular example, it looks like the accuracy is a meaningful metric to use for assessment. If the accuracy is very high (above $90\\%$ for example), we say that our model is performing good. Otherwise, we say that our model's performance is bad. However, the notion of accuracy is not always a good metric to look at and it can be very misleading as we will see soon. In some specific problems, having an accuracy as high as $99\\%$ does not necessarily mean that our model is performing good. Hence, we need to define other metrics to assess the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Accuracy is not Always a Good Metric!\n",
    "\n",
    "By examining a real-world classification example, we will see that the accuracy is not always enough to assess the performance, especially when dealing with **imbalanced** dataset or trying to avoid a special type of errors.\n",
    "\n",
    "<u>Example: binary classification of a rare fatal disease (imbalanced dataset and low miss-classification rate of disease is required):</u>\n",
    "<br>\n",
    "Assume we are doing binary classification based on some medical data of the patients in order to predict whether the patient has the \"corona virus\" (positive) or not (negative). The corona virus is a rare disease. Hence, we expect that our datasets (both the training and the testing) are highly imbalanced, which means that we expect to have much more negative than positive.  After training three different classifiers, we test them on 1000 instances among them 990 are negative and only 10 are positive. The classifiers give the following predictions:\n",
    "\n",
    "* Classifier 1 (SVM): predicts 950 negative (among them 945 are correct predictions) and 50 positive (among them 5 are correct predictions). The accuracy is $(945 + 5)/1000 = 95\\%$.\n",
    "* Classifier 2 (logistic regression): predicts 958 negative (among them 949 are correct) and 42 positive (among them 1 is correct). The accuracy is $(949+1)/1000 = 95\\%$.\n",
    "* Classifier 3 (deterministic): always predicts negative without looking into the data. Thus, it outputs 1000 negative. The accuracy is $99\\%$.\n",
    "\n",
    "Let's try to assess the performance of these three classifiers. Classifier 1 and 2 have the same accuracy but it is clear that they perform differently. Both classifiers miss-classified 50 patients. However, the type of errors are different which have  different effects for such application. Classifier 1 was able to detect half of the corona virus while classifier 2 detected only one case. The risk of failing to detect the fatal corona virus on a sick patient is much higher than the risk of asking a healthy patient to do additional diagnosis. Hence, classifier 1 looks safer for such application. This difference is not reflected in the accuracy metric. Another very important shortcoming of the accuracy metric appears when we look at the accuracy of classifier 3. This \"dumb\" classifier does literally nothing, it always predicts the negative class whatever the input is. Yet, classifier 3 seems to attain much better accuracy than the first two. You can conclude now that the accuracy measure is not enough to distinguish between the first two classifiers (even if the dataset is balanced). Moreover, it gives a misleading impression about the performance of the third classifier.\n",
    "\n",
    "<!---<u>Example 2: Binary classification of fatal disease (low miss-classification rate of disease is required):</u>\n",
    "<br>\n",
    "We will illustrate here another shortcoming of the accuracy metric that arises even for balanced datasets. Assume we are doing binary classification based on some medical data of the patients in order to predict whether the patient has the \"corona virus\" (positive) or not (negative). Assume further that our dataset is somehow balanced. After training two different classifiers, we test them on 1000 instances among them 500 are positive. The classifiers give the following predictions:\n",
    "\n",
    "* Classifier 1 (SVM): predicts 600 positive (among them 500 are correct) and 400 negative (all of them are correct). The accuracy is $(500 + 400)/1000 = 90\\%$.\n",
    "* Classifier 2 (logistic regression): predicts 400 positive (all of them are correct) and 600 negative (among them 500 are correct). The accuracy is $(400+500)/1000 = 90\\%$.\n",
    "\n",
    "Classifier 1 and 2 have the same accuracy but their performances are clearly different. Both classifiers miss-classified 100 patients. However, the type of errors are different which have  different effects for such application. The cost of failing to detect the fatal corona virus on a sick patient is much higher than the cost of sending a healthy patient to more tests. Hence, classifier 1 looks safer for such application. This difference is not reflected in the accuracy metric.--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Performance Metrics: Recall, Precision, and F-1 Score\n",
    "\n",
    "We will define now three additional metrics that help in a better performance evaluation and in analyzing some additional characteristics not reflected in the accuracy metric.\n",
    "\n",
    "<img src=\"balanced.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a binary classification, assume we have excess to the ground truth labels: the positive instances (P) and the negative ones (N). After training our preferred classifier on the training dataset and applying it to the test dataset, our classifier splits the dataset into four event as shown in the conceptual example of the figure above (both for balanced and imbalanced datasets): true positive (TP), true negative (TN), false positive (FP), and false negative (FN). Notice that our famous accuracy metric is nothing but $(TP+TN)/(P+N)$. We will define two additional metrics now:\n",
    "\n",
    "* Recall (or sensitivity or true positive rate \"TPR\"): $recall = TP/P = TP/(TP+FN)$.\n",
    "* Precision (or positive predictive value \"PPV\"): $precision = TP/(TP+FP)$.\n",
    "\n",
    "In fact, what we want is to make both FN and FP as small as possible while having low sensitivity to the class imbalance when exists (i.e. without including the TN). The recall and precision provide this for us! The recall reflects the FN and the precision reflects the FP. Hence, we need to have both the recall and precision as high as possible. What if we merge these two metrics in one metric?\n",
    "\n",
    "F-1 score combines the recall and precision in a weighted average. Therefore, F-1 takes both false positives and false negatives into account (it is the harmonic mean of recall and precision):\n",
    "\n",
    "$$F1 = 2 (recall \\times precision) / (recall + precision)$$\n",
    "\n",
    "Let's go back to our corona virus example and evaluate the performance with the new metrics:\n",
    "* Classifier 1: accuracy 95%, recall 50%, precision 10%, F-1 score 0.167.\n",
    "* Classifier 2: accuracy 95%, recall 10%, precision 23.8%, F-1 score 0.141.\n",
    "* Classifier 3: accuracy 99%, recall 0%, precision 0%, F-1 score 0.\n",
    "\n",
    "You can see that classifier 1 is the preferred model since it has the highest F-1 score, although it does not have the highest accuracy. Since classifier 1 is better in recall and classifier 2 is better in precision, it is hard to make a comparison. F-1 score combines the two metrics and gives a unique score that helps us decide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Scikit-learn Demo (Breast Cancer Dataset)\n",
    "\n",
    "In this demo, we will illustrate how to evaluate the performance of a binary classifier on the \"breast cancer\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary module from scikit learn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# split data for traing and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a linear binary classifier using SVM\n",
    "model = LinearSVC(random_state=15).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels of the test dataset\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now evaluate the performance of our model. First we want to compute all the four events shown in the figure above (i.e. TN, FP, FN, TP). Scikit-learn allows this via the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: \n",
      " [[67  0]\n",
      " [32 89]]\n"
     ]
    }
   ],
   "source": [
    "# compute the confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, predictions)\n",
    "print(\"confusion matrix: \\n\", conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN =  67 FP =  0 FN =  32 TP =  89\n"
     ]
    }
   ],
   "source": [
    "# pritn the four events\n",
    "tn, fp, fn, tp = conf_mat.ravel()\n",
    "print(\"TN = \", tn, \"FP = \", fp, \"FN = \", fn, \"TP = \", tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compute the accuracy, recall, precision, and F-1 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy =  82.97872340425532 %\n",
      " Recall =  73.55371900826447 %\n",
      " Precision =  100.0 %\n",
      " F-1 score =  0.8476190476190476\n"
     ]
    }
   ],
   "source": [
    "print(\" Accuracy = \", acc*100, \"%\\n\", \"Recall = \", recall*100, \"%\\n\", \"Precision = \", precision*100, \"%\\n\",\n",
    "      \"F-1 score = \", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
